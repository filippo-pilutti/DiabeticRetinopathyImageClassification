{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TODO\n",
        "- DONE preprocess su immagini di dimensioni reali, e resizing successivo\n",
        "- migliorare data augmentation in modo che le immagini vengano molto più diverse una dall'altra"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare all the dataset\n",
        "Before using the following code, install necessary libraries\n",
        "'pip install pandas opencv-python numpy matplotlib pillow tqdm torch torchvision torchinfo scikit-learn focal-loss-torch'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGwcx61F9Ove"
      },
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# Librerie standard Python\n",
        "# =========================================================\n",
        "import os\n",
        "import gc\n",
        "import io\n",
        "import csv\n",
        "import json\n",
        "import base64\n",
        "import shutil\n",
        "import random\n",
        "import pathlib\n",
        "import subprocess\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "from typing import Optional, Dict, Any, List, Callable\n",
        "from collections import Counter\n",
        "\n",
        "# =========================================================\n",
        "# Librerie scientifiche e di elaborazione dati\n",
        "# =========================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, confusion_matrix,\n",
        "    precision_score, recall_score, f1_score\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# Librerie per immagini e visualizzazione\n",
        "# =========================================================\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# =========================================================\n",
        "# PyTorch e moduli correlati\n",
        "# =========================================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
        "\n",
        "# =========================================================\n",
        "# TorchVision\n",
        "# =========================================================\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torchvision.transforms import functional\n",
        "\n",
        "# =========================================================\n",
        "# Altre librerie\n",
        "# =========================================================\n",
        "from torchinfo import summary\n",
        "from focal_loss.focal_loss import FocalLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZbOgqutEaNL9"
      },
      "outputs": [],
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # Unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_resized = 1\n",
        "local_runtime = 1\n",
        "download_data_from_drive = 0\n",
        "use_data_preprocessing = 0\n",
        "use_data_augmentation = 0\n",
        "use_subset_loader = 1\n",
        "preprocess_out_size = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_size = 224\n",
        "batch_size = 64\n",
        "\n",
        "# Target per classe della data augmentation: se None, usa la massima numerosità corrente, altrimenti\n",
        "# genera un numero di immagini pari al numero scelto (1000) - quante ce ne sono già\n",
        "TARGET_PER_CLASS = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "cXjdzS0Y9_Sc"
      },
      "outputs": [],
      "source": [
        "if use_resized == 1:\n",
        "  resized_path = 'resized_512/'\n",
        "else:\n",
        "  resized_path = \"\"\n",
        "drive_zip_path = \"/content/drive/MyDrive/COMPUTER VISION PROJECT/dataset/resized_512/\"\n",
        "\n",
        "if local_runtime==1:\n",
        "    #base_path = \"/Users/ire/Documents/2° Year/SELECTED TOPICS /project/dataset/\"\n",
        "    #base_path = 'C:/Users/s.simonitti/Desktop/SelectedTopicsInML/dataset/'\n",
        "    base_path = \"C:/Users/simon/Desktop/DiabeticRetinopathyImageClassification/dataset/\"\n",
        "else:\n",
        "    base_path = '/content/drive/MyDrive/Colab Notebooks/dataset/'\n",
        "\n",
        "data_dir = base_path + resized_path + 'train_images'\n",
        "val_dir = base_path + resized_path + 'val_images'\n",
        "test_dir = base_path + resized_path + 'test_images'\n",
        "processed_train_dir = base_path + \"processed_dataset/train_images\"\n",
        "processed_val_dir = base_path + \"processed_dataset/val_images\"\n",
        "processed_test_dir = base_path + \"processed_dataset/test_images\"\n",
        "\n",
        "if use_data_augmentation==1:\n",
        "  augm_path=\"augmented_dataset/\"\n",
        "else:\n",
        "  augm_path=\"\"\n",
        "\n",
        "augm_train_dir = base_path + augm_path + \"train_images\"\n",
        "\n",
        "\n",
        "# csv file names\n",
        "train_binary_file = \"train_binary.csv\"\n",
        "train_four_classes_file = \"train_four_classes.csv\"\n",
        "\n",
        "val_binary_file = \"val_binary.csv\"\n",
        "val_four_classes_file = \"val_four_classes.csv\"\n",
        "\n",
        "test_binary_file = \"test_binary.csv\"\n",
        "test_four_classes_file = \"test_four_classes.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if local_runtime==0:\n",
        "    drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download zip dataset from drive and unzip it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFCMkcCR4EZl"
      },
      "outputs": [],
      "source": [
        "import os, shutil, pathlib, subprocess\n",
        "\n",
        "def prepare_dataset_zip(\n",
        "    drive_zip_path: str,\n",
        "    local_zip_path: str,\n",
        "    extract_dir: str,\n",
        "    ready_flag: str = \".ready\",\n",
        "    verbose: bool = True,\n",
        "    delete_zip_after_extract: bool = True,\n",
        "):\n",
        "    \"\"\"\n",
        "    Copia uno zip da Google Drive, lo estrae localmente in /content,\n",
        "    e cancella lo zip dopo l'estrazione. Usa un file sentinella (.ready)\n",
        "    per evitare di ripetere l'estrazione se già completata.\n",
        "\n",
        "    Args:\n",
        "        drive_zip_path (str): percorso completo allo zip su Drive.\n",
        "        local_zip_path (str): percorso temporaneo per lo zip locale.\n",
        "        extract_dir (str): cartella dove estrarre i file.\n",
        "        ready_flag (str): nome del file sentinella per la cache.\n",
        "        verbose (bool): stampa messaggi di stato se True.\n",
        "        delete_zip_after_extract (bool): se True, rimuove lo zip dopo l'estrazione.\n",
        "\n",
        "    Returns:\n",
        "        str: percorso della cartella estratta (extract_dir)\n",
        "    \"\"\"\n",
        "    extract_dir = os.path.abspath(extract_dir)\n",
        "    ready_file = os.path.join(extract_dir, ready_flag)\n",
        "    pathlib.Path(extract_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if not os.path.exists(ready_file):\n",
        "        if verbose: print(\"Copio lo zip da Drive a /content...\")\n",
        "        shutil.copy(drive_zip_path, local_zip_path)\n",
        "\n",
        "        if verbose: print(\"Estrazione in corso...\")\n",
        "        subprocess.run([\"unzip\", \"-q\", \"-n\", local_zip_path, \"-d\", extract_dir], check=True)\n",
        "\n",
        "        # crea file sentinella\n",
        "        open(ready_file, \"w\").close()\n",
        "        if verbose: print(f\"Dataset pronto in {extract_dir}\")\n",
        "\n",
        "        # rimuove zip locale per liberare spazio\n",
        "        if delete_zip_after_extract and os.path.exists(local_zip_path):\n",
        "            os.remove(local_zip_path)\n",
        "            if verbose: print(f\"File zip locale rimosso: {local_zip_path}\")\n",
        "    else:\n",
        "        if verbose: print(f\"Dataset già estratto in {extract_dir}, uso cache locale.\")\n",
        "\n",
        "    return extract_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGYGbbxTvSPb",
        "outputId": "fcd2e118-518b-4134-a6d3-382f5a87875f"
      },
      "outputs": [],
      "source": [
        "if download_data_from_drive:\n",
        "    data_dir = prepare_dataset_zip(\n",
        "        drive_zip_path=drive_zip_path + \"train_images.zip\",\n",
        "        local_zip_path=base_path + \"train_images.zip\",\n",
        "        extract_dir=data_dir\n",
        "    )\n",
        "    val_dir = prepare_dataset_zip(\n",
        "        drive_zip_path=drive_zip_path + \"val_images.zip\",\n",
        "        local_zip_path=base_path + \"val_images.zip\",\n",
        "        extract_dir=val_dir\n",
        "    )\n",
        "    test_dir = prepare_dataset_zip(\n",
        "        drive_zip_path=drive_zip_path + \"test_images.zip\",\n",
        "        local_zip_path=base_path + \"test_images.zip\",\n",
        "        extract_dir=test_dir\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "train_images = os.listdir(data_dir)\n",
        "val_images = os.listdir(val_dir)\n",
        "test_images = os.listdir(test_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "I33iWpgS-JBA",
        "outputId": "21502196-32c9-47c5-9dde-44ae261db4ee"
      },
      "outputs": [],
      "source": [
        "img = cv2.imread(os.path.join(data_dir, train_images[700]))\n",
        "if img is not None:\n",
        "    # Argomento 1: Nome della finestra (stringa) | Argomento 2: Immagine (matrice)\n",
        "    cv2.imshow(\"Immagine Retina\", img) \n",
        "    \n",
        "    # Aspetta che venga premuto un tasto (il valore 0 significa per sempre, fino a una pressione)\n",
        "    #cv2.waitKey(0) \n",
        "    \n",
        "    # Chiude tutte le finestre di visualizzazione di OpenCV\n",
        "    cv2.destroyAllWindows() \n",
        "else:\n",
        "    print(\"Errore nel caricamento dell'immagine. Controlla il percorso o il file.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuun_T1jAOad",
        "outputId": "ff5a678a-7d1c-4280-ec5f-7f88ff0b7b57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(512, 512, 3)\n"
          ]
        }
      ],
      "source": [
        "print(img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess images\n",
        "Denoise and other"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_retina_image(image_path):\n",
        "    # 1. Load image (RGB)\n",
        "    img = cv2.imread(image_path)\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 2. Cropping eye region (remove black background)\n",
        "    gray = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2GRAY)\n",
        "    _, thresh = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY)\n",
        "    contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    # Find largest contour (eye region)\n",
        "    c = max(contours, key=cv2.contourArea)\n",
        "    x, y, w, h = cv2.boundingRect(c)\n",
        "    cropped_img = img_rgb[y:y+h, x:x+w]\n",
        "\n",
        "    # 3. Denoising (Gaussian Blur, kernel size 3x3)\n",
        "    denoised_img = cv2.GaussianBlur(cropped_img, (3, 3), 0)\n",
        "\n",
        "    # 4. Histogram Equalization on Y channel (YUV color space)\n",
        "    img_yuv = cv2.cvtColor(denoised_img, cv2.COLOR_RGB2YUV)\n",
        "    img_yuv[:,:,0] = cv2.equalizeHist(img_yuv[:,:,0])\n",
        "    he_img = cv2.cvtColor(img_yuv, cv2.COLOR_YUV2RGB)\n",
        "\n",
        "    final_img = cv2.resize(he_img, (preprocess_out_size, preprocess_out_size))\n",
        "\n",
        "    return final_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_image_batch(\n",
        "    images_to_process,\n",
        "    images_dir,\n",
        "    out_images_dir\n",
        "):\n",
        "    for img_name in tqdm(images_to_process, desc=\"Processing Images\", unit=\"image\"):\n",
        "        # 1. Determina immediatamente il percorso di salvataggio\n",
        "        save_path = os.path.join(out_images_dir, img_name)\n",
        "\n",
        "        # 2. CONTROLLO DI ESISTENZA\n",
        "        if os.path.exists(save_path):\n",
        "            # Se il file esiste, stampa un messaggio e passa all'immagine successiva\n",
        "            # print(f\"Skipping {img_name}: already processed.\")\n",
        "            continue # Passa all'elemento successivo nel ciclo\n",
        "    \n",
        "        # Se il file NON esiste, esegui il processo\n",
        "        img_path = os.path.join(images_dir, img_name)\n",
        "    \n",
        "        try:\n",
        "            # Elaborazione (viene eseguita solo se il file non esiste)\n",
        "            processed_img = preprocess_retina_image(img_path)\n",
        "        \n",
        "            # Salvataggio\n",
        "            cv2.imwrite(save_path, cv2.cvtColor(processed_img, cv2.COLOR_RGB2BGR))\n",
        "        \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {img_name}: {e}\")\n",
        "\n",
        "    print(f\"Finished processing and saving images to {out_images_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "if use_data_preprocessing:\n",
        "    os.makedirs(processed_train_dir, exist_ok=True)\n",
        "    os.makedirs(processed_val_dir, exist_ok=True)\n",
        "    os.makedirs(processed_test_dir, exist_ok=True)\n",
        "\n",
        "    print(\"Processing training images...\")\n",
        "    process_image_batch(\n",
        "        images_to_process = train_images,\n",
        "        images_dir = data_dir,\n",
        "        out_images_dir = processed_train_dir)\n",
        "\n",
        "    print(\"Processing vaòidation images...\")\n",
        "    process_image_batch(\n",
        "        images_to_process = val_images,\n",
        "        images_dir = val_dir,\n",
        "        out_images_dir = processed_val_dir)\n",
        "\n",
        "    print(\"Processing testing images...\")\n",
        "    process_image_batch(\n",
        "        images_to_process = test_images,\n",
        "        images_dir = test_dir,\n",
        "        out_images_dir = processed_test_dir)\n",
        "else:\n",
        "    processed_train_dir = data_dir\n",
        "    processed_val_dir = val_dir\n",
        "    processed_test_dir = test_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare binary classification labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crea_file_diagnosis_binaria(input_path: str, output_path: str):\n",
        "    \"\"\"\n",
        "    Legge un file CSV/XLS con una colonna 'diagnosis' e crea un nuovo file\n",
        "    in cui 'diagnosis' vale 0 se è 0, altrimenti 1.\n",
        "    \n",
        "    :param input_path: percorso del file di input (CSV o Excel)\n",
        "    :param output_path: percorso del file di output CSV\n",
        "    \"\"\"\n",
        "    # Legge il file (rileva automaticamente se è CSV o Excel)\n",
        "    df = pd.read_csv(input_path)\n",
        "\n",
        "    # Conversione della colonna 'diagnosis' in 0/1\n",
        "    df['diagnosis'] = df['diagnosis'].apply(lambda x: 0 if x == 0 else 1)\n",
        "\n",
        "    # Salva il nuovo file\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"File con diagnosis binaria salvato in: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File con diagnosis binaria salvato in: C:/Users/simon/Desktop/DiabeticRetinopathyImageClassification/dataset/train_binary.csv\n",
            "File con diagnosis binaria salvato in: C:/Users/simon/Desktop/DiabeticRetinopathyImageClassification/dataset/val_binary.csv\n",
            "File con diagnosis binaria salvato in: C:/Users/simon/Desktop/DiabeticRetinopathyImageClassification/dataset/test_binary.csv\n"
          ]
        }
      ],
      "source": [
        "crea_file_diagnosis_binaria(input_path=base_path + \"train.csv.xls\", output_path=base_path + train_binary_file)\n",
        "crea_file_diagnosis_binaria(input_path=base_path + \"val.csv.xls\", output_path=base_path + val_binary_file)\n",
        "crea_file_diagnosis_binaria(input_path=base_path + \"test.csv.xls\", output_path=base_path + test_binary_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare four classes labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crea_file_diagnosis_nonzero(input_path: str, output_path: str):\n",
        "    \"\"\"\n",
        "    Legge un file CSV/XLS con una colonna 'diagnosis' e crea un nuovo file\n",
        "    contenente solo le righe dove 'diagnosis' è diversa da 0.\n",
        "    \n",
        "    :param input_path: percorso del file di input (CSV o Excel)\n",
        "    :param output_path: percorso del file di output CSV\n",
        "    \"\"\"\n",
        "    # Legge il file (rileva automaticamente se è CSV o Excel)\n",
        "    df = pd.read_csv(input_path)\n",
        "\n",
        "    # Filtra solo le righe con diagnosis diversa da 0\n",
        "    df_filtrato = df[df['diagnosis'] != 0].copy()\n",
        "    df_filtrato['diagnosis'] = df_filtrato['diagnosis'].apply(lambda x: x-1)\n",
        "\n",
        "    # Salva il nuovo file\n",
        "    df_filtrato.to_csv(output_path, index=False)\n",
        "    print(f\"File con diagnosis != 0 salvato in: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File con diagnosis != 0 salvato in: C:/Users/simon/Desktop/DiabeticRetinopathyImageClassification/dataset/train_four_classes.csv\n",
            "File con diagnosis != 0 salvato in: C:/Users/simon/Desktop/DiabeticRetinopathyImageClassification/dataset/val_four_classes.csv\n",
            "File con diagnosis != 0 salvato in: C:/Users/simon/Desktop/DiabeticRetinopathyImageClassification/dataset/test_four_classes.csv\n"
          ]
        }
      ],
      "source": [
        "crea_file_diagnosis_nonzero(input_path=base_path + \"train.csv.xls\", output_path=base_path + train_four_classes_file)\n",
        "crea_file_diagnosis_nonzero(input_path=base_path + \"val.csv.xls\", output_path=base_path + val_four_classes_file)\n",
        "crea_file_diagnosis_nonzero(input_path=base_path + \"test.csv.xls\", output_path=base_path + test_four_classes_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FundusAugment:\n",
        "    \"\"\"\n",
        "    Augmentations secondo Tabella 3:\n",
        "    - Zoom range: 0.2  -> scale (0.8, 1.0)\n",
        "    - Rotation range: ±10°\n",
        "    - Flip: orizzontale + verticale\n",
        "    - Brightness/Color(=Saturation)/Contrast: (0.5, 1.5)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: 512,\n",
        "        p_zoom: float = 0.7,\n",
        "        p_hflip: float = 0.5,\n",
        "        p_vflip: float = 0.5,\n",
        "        p_rotate: float = 1,\n",
        "        p_color: float = 1,\n",
        "        zoom_scale=(0.80, 1.00),\n",
        "        zoom_ratio=(0.95, 1.05),\n",
        "        rot_deg: float = 20.0,\n",
        "        brightness=(0.5, 1.5),\n",
        "        contrast=(0.5, 1.5),\n",
        "        color_adjustment=(0.5, 1.5)\n",
        "    ):\n",
        "        self.img_size = img_size\n",
        "        self.p_zoom = p_zoom\n",
        "        self.p_hflip = p_hflip\n",
        "        self.p_vflip = p_vflip\n",
        "        self.p_rotate = p_rotate\n",
        "        self.p_color = p_color\n",
        "        self.zoom_scale = zoom_scale\n",
        "        self.zoom_ratio = zoom_ratio\n",
        "        self.rot_deg = rot_deg\n",
        "        self.resize = transforms.Resize((img_size, img_size))\n",
        "\n",
        "        self.color_jitter = transforms.ColorJitter(\n",
        "            brightness=brightness,\n",
        "            contrast=contrast,\n",
        "            saturation=color_adjustment,\n",
        "            hue=0.0\n",
        "        )\n",
        "\n",
        "    def __call__(self, img: Image.Image) -> Image.Image:\n",
        "        img = self.resize(img)\n",
        "\n",
        "        if random.random() < self.p_zoom:\n",
        "            rrc = transforms.RandomResizedCrop(\n",
        "                self.img_size, scale=self.zoom_scale#, ratio=self.zoom_ratio\n",
        "            )\n",
        "            img = rrc(img)\n",
        "\n",
        "        if random.random() < self.p_hflip:\n",
        "            img = functional.hflip(img)\n",
        "        if random.random() < self.p_vflip:\n",
        "            img = functional.vflip(img)\n",
        "\n",
        "        if random.random() < self.p_rotate:\n",
        "            angle = random.uniform(-self.rot_deg, self.rot_deg)\n",
        "            img = functional.rotate(img, angle, interpolation=Image.BICUBIC, expand=False, fill=0)\n",
        "\n",
        "        if random.random() < self.p_color:\n",
        "            img = self.color_jitter(img)\n",
        "\n",
        "        return img\n",
        "  \n",
        "# -----------------------------\n",
        "# UTILS\n",
        "# -----------------------------\n",
        "EXTS = [\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\", \".bmp\", \".webp\"]\n",
        "\n",
        "def find_image_for_id(id_code: str, root: Path) -> Path | None:\n",
        "    for ext in EXTS:\n",
        "        p = root / f\"{id_code}{ext}\"\n",
        "        if p.exists():\n",
        "            return p\n",
        "    # fallback: cerca per pattern (es. file con id_code come prefisso)\n",
        "    matches = list(root.glob(f\"{id_code}.*\"))\n",
        "    return matches[0] if matches else None\n",
        "\n",
        "def load_df(csv_path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(csv_path)\n",
        "    cols = [c.lower() for c in df.columns]\n",
        "    # normalizza nomi colonne attese\n",
        "    if \"id_code\" in cols and \"diagnosis\" in cols:\n",
        "        return df.rename(columns={df.columns[cols.index(\"id_code\")]: \"id_code\",\n",
        "                                  df.columns[cols.index(\"diagnosis\")]: \"diagnosis\"})\n",
        "    elif \"filepath\" in cols and \"diagnosis\" in cols:\n",
        "        return df.rename(columns={df.columns[cols.index(\"filepath\")]: \"filepath\",\n",
        "                                  df.columns[cols.index(\"diagnosis\")]: \"diagnosis\"})\n",
        "    else:\n",
        "        raise ValueError(\"CSV deve avere colonne (id_code,diagnosis) oppure (filepath,diagnosis).\")\n",
        "\n",
        "def row_to_path(row, dir_img) -> Path:\n",
        "    if \"filepath\" in row and isinstance(row[\"filepath\"], str):\n",
        "        return Path(row[\"filepath\"])\n",
        "    # se abbiamo id_code, cerchiamo il file nella cartella root\n",
        "    return find_image_for_id(str(row[\"id_code\"]), dir_img)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "if use_data_augmentation==1:\n",
        "  # Percorsi\n",
        "  TRAIN_IMG_DIR = Path(processed_train_dir)\n",
        "  TRAIN_CSV_IN  = Path(base_path + train_four_classes_file)\n",
        "  TRAIN_CSV_OUT = Path(base_path + augm_path + \"train.csv.xls\")\n",
        "  # Dove salvare i nuovi file\n",
        "  OUT_IMG_DIR = Path(augm_train_dir)\n",
        "  # -----------------------------\n",
        "  # CONFIG\n",
        "  # -----------------------------\n",
        "  SEED = 42\n",
        "  random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "  # Modalità CSV out: \"w\" per riscrivere tutto (originali + augment), \"a\" per appendere solo le nuove righe\n",
        "  CSV_MODE = \"w\"\n",
        "  WRITE_HEADER = True if CSV_MODE == \"w\" else False\n",
        "  \n",
        "  # Se la cartella di output contiene già immagini aumentate, salta tutto\n",
        "  if OUT_IMG_DIR.exists() and any(OUT_IMG_DIR.glob(\"*.png\")):\n",
        "      print(f\"Data augmentation già eseguita: trovate immagini in {OUT_IMG_DIR}.\")\n",
        "      print(\"Salto la rigenerazione e uso i file esistenti.\")\n",
        "  else:\n",
        "    OUT_IMG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    # Colonna per l'output: \"id_code\" (consigliato se il tuo Dataset fa f\"{id_code}.jpg\") oppure \"filepath\"\n",
        "    OUTPUT_USES_ID_CODE = True\n",
        "\n",
        "    augment = FundusAugment(img_size=preprocess_out_size)\n",
        "\n",
        "    # -----------------------------\n",
        "    # CARICA CSV E CONTA PER CLASSE\n",
        "    # -----------------------------\n",
        "    df_in = load_df(TRAIN_CSV_IN)\n",
        "    df_in[\"diagnosis\"] = df_in[\"diagnosis\"].astype(int)\n",
        "\n",
        "    # calcola counts per classe dal CSV\n",
        "    counts = df_in.groupby(\"diagnosis\").size().to_dict()\n",
        "    all_classes = sorted(df_in[\"diagnosis\"].unique().tolist())\n",
        "    if TARGET_PER_CLASS is None:\n",
        "      TARGET_PER_CLASS = max(counts.values())\n",
        "\n",
        "    print(\"Conteggi iniziali:\", counts)\n",
        "    print(\"Target per classe:\", TARGET_PER_CLASS)\n",
        "\n",
        "    # mappa: classe -> lista (id_code, path)\n",
        "    by_class = {c: [] for c in all_classes}\n",
        "    for _, row in df_in.iterrows():\n",
        "      p = row_to_path(row, TRAIN_IMG_DIR)\n",
        "      if p is None or not p.exists():\n",
        "        # salta righe orfane\n",
        "        continue\n",
        "      if \"id_code\" in df_in.columns:\n",
        "        by_class[row[\"diagnosis\"]].append((str(row[\"id_code\"]), p))\n",
        "      else:\n",
        "        # crea un id_code dal filename (senza estensione)\n",
        "        by_class[row[\"diagnosis\"]].append((p.stem, p))\n",
        "\n",
        "    # contatori per generare suffissi univoci per ciascun id_code base\n",
        "    per_id_counters = {}\n",
        "\n",
        "    # -----------------------------\n",
        "    # PREPARA CSV DI OUTPUT\n",
        "    # -----------------------------\n",
        "    TRAIN_CSV_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "    mode = CSV_MODE\n",
        "    write_header = WRITE_HEADER\n",
        "    fout = open(TRAIN_CSV_OUT, mode, newline=\"\", encoding=\"utf-8\")\n",
        "    writer = csv.writer(fout)\n",
        "\n",
        "    # Decidi intestazioni e funzione che scrive una riga\n",
        "    if OUTPUT_USES_ID_CODE:\n",
        "      if write_header:\n",
        "        writer.writerow([\"id_code\", \"diagnosis\"])\n",
        "      def write_row_id(id_code: str, diagnosis: int):\n",
        "        writer.writerow([id_code, diagnosis])\n",
        "    else:\n",
        "      if write_header:\n",
        "        writer.writerow([\"filepath\", \"diagnosis\"])\n",
        "      def write_row_path(path: Path, diagnosis: int):\n",
        "        writer.writerow([str(path), diagnosis])\n",
        "\n",
        "  # se stai riscrivendo tutto, copia anche le righe originali nel nuovo CSV\n",
        "    if mode == \"w\":\n",
        "      for c in all_classes:\n",
        "          for id_code, path in by_class[c]:\n",
        "            if OUTPUT_USES_ID_CODE:\n",
        "                write_row_id(id_code, c)\n",
        "            else:\n",
        "                write_row_path(path, c)\n",
        "\n",
        "  # -----------------------------\n",
        "  # GENERA AUGMENT PER RAGGIUNGERE IL TARGET\n",
        "  # -----------------------------\n",
        "    for c in all_classes:\n",
        "      current = len(by_class[c])\n",
        "      need = max(0, TARGET_PER_CLASS - current)\n",
        "      if need == 0:\n",
        "        continue\n",
        "\n",
        "      print(f\"Classe {c}: genero {need} immagini…\")\n",
        "      src_items = by_class[c]\n",
        "      if not src_items:\n",
        "        continue\n",
        "\n",
        "      for i in tqdm(range(need), desc=f\"Augment class {c}\"):\n",
        "        base_id, src_path = random.choice(src_items)\n",
        "        with Image.open(src_path) as im:\n",
        "            im = im.convert(\"RGB\")\n",
        "            aug_img = augment(im)\n",
        "\n",
        "        # genera nome univoco\n",
        "        k = per_id_counters.get(base_id, 0)\n",
        "        per_id_counters[base_id] = k + 1\n",
        "        new_id = f\"{base_id}aug{k:05d}\"\n",
        "        out_path = OUT_IMG_DIR / f\"{new_id}.png\"\n",
        "\n",
        "        # evita collisioni nel raro caso il file esista\n",
        "        while out_path.exists():\n",
        "            k += 1\n",
        "            per_id_counters[base_id] = k + 1\n",
        "            new_id = f\"{base_id}aug{k:05d}\"\n",
        "            out_path = OUT_IMG_DIR / f\"{new_id}.png\"\n",
        "\n",
        "        aug_img.save(out_path, format=\"PNG\", quality=92, optimize=True, progressive=True)\n",
        "\n",
        "        # scrivi riga CSV per la nuova immagine\n",
        "        if OUTPUT_USES_ID_CODE:\n",
        "            write_row_id(new_id, c)\n",
        "        else:\n",
        "            write_row_path(out_path, c)\n",
        "\n",
        "    src = Path(processed_train_dir)\n",
        "    dst = Path(augm_train_dir)\n",
        "\n",
        "    # Copia tutto il contenuto (anche sottocartelle e file)\n",
        "    shutil.copytree(src, dst, dirs_exist_ok=True)\n",
        "    fout.close()\n",
        "    print(f\"Immagini nuove in: {OUT_IMG_DIR}\")\n",
        "    print(f\"CSV scritto in: {TRAIN_CSV_OUT}\")\n",
        "else:\n",
        "  augm_train_dir = processed_train_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load image names and labes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "vn59LssWGnD7"
      },
      "outputs": [],
      "source": [
        "labels_train_binary = pd.read_csv(base_path + augm_path + train_binary_file)\n",
        "labels_val_binary = pd.read_csv(base_path + val_binary_file)\n",
        "labels_test_binary = pd.read_csv(base_path + test_binary_file)\n",
        "\n",
        "labels_train_four_classes = pd.read_csv(base_path + augm_path + train_four_classes_file)\n",
        "labels_val_four_classes = pd.read_csv(base_path + val_four_classes_file)\n",
        "labels_test_four_classes = pd.read_csv(base_path + test_four_classes_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_dataset_statistics(msg: str, train_labels, val_labels, test_labels):\n",
        "    table_cnt = pd.DataFrame({\n",
        "        'TRAIN': train_labels['diagnosis'].value_counts().sort_index(),\n",
        "        'TEST': val_labels['diagnosis'].value_counts().sort_index(),\n",
        "        'VALIDATION': test_labels['diagnosis'].value_counts().sort_index()\n",
        "    }).T\n",
        "\n",
        "    table_perc = pd.DataFrame({\n",
        "    'TRAIN': round(train_labels['diagnosis'].value_counts(normalize=True).sort_index() * 100, 3),\n",
        "    'TEST': round(val_labels['diagnosis'].value_counts(normalize=True).sort_index() * 100, 3),\n",
        "    'VALIDATION': round(test_labels['diagnosis'].value_counts(normalize=True).sort_index() * 100, 3)\n",
        "    }).T  # trasponi per avere i dataset come righe\n",
        "\n",
        "    # Mostra le tabella\n",
        "    print(msg)\n",
        "    print(\"Dataset count\")\n",
        "    print(table_cnt)\n",
        "    print(\"\\nDataset percentage\")\n",
        "    print(table_perc)\n",
        "\n",
        "    train_labels['diagnosis'].value_counts().plot(kind = 'bar', color = ['red', 'blue', 'green', 'cyan', 'purple'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary dataset:\n",
            "Dataset count\n",
            "diagnosis      0     1\n",
            "TRAIN       1434  1496\n",
            "TEST         172   194\n",
            "VALIDATION   199   167\n",
            "\n",
            "Dataset percentage\n",
            "diagnosis        0       1\n",
            "TRAIN       48.942  51.058\n",
            "TEST        46.995  53.005\n",
            "VALIDATION  54.372  45.628\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_dataset_statistics(\"Binary dataset:\", labels_train_binary, labels_val_binary, labels_test_binary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Four classes dataset:\n",
            "Dataset count\n",
            "diagnosis     1    2    3    4\n",
            "TRAIN       300  808  154  234\n",
            "TEST         40  104   22   28\n",
            "VALIDATION   30   87   17   33\n",
            "\n",
            "Dataset percentage\n",
            "diagnosis        1       2       3       4\n",
            "TRAIN       20.053  54.011  10.294  15.642\n",
            "TEST        20.619  53.608  11.340  14.433\n",
            "VALIDATION  17.964  52.096  10.180  19.760\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "show_dataset_statistics(\"Four classes dataset:\", labels_train_four_classes, labels_val_four_classes, labels_test_four_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "P9CSU5YYF9mU"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((img_size, img_size)),\n",
        "    #transforms.CenterCrop(img_size),\n",
        "    transforms.ToTensor(),               # convert to tensor [0,1]\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],      # ImageNet mean\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define custom Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "HW_TbWciGIX-"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, img_dir, labels_df, transform=None, preload=True):\n",
        "        self.img_dir = img_dir\n",
        "        self.labels_df = labels_df\n",
        "        self.transform = transform\n",
        "        self.preload = preload\n",
        "\n",
        "        # Map filenames and labels\n",
        "        self.filenames = self.labels_df['id_code'].values\n",
        "        self.labels = self.labels_df['diagnosis'].values\n",
        "\n",
        "        self.images = []  # qui salveremo le immagini pre-caricate\n",
        "\n",
        "        if self.preload:\n",
        "            print(\"Caricamento immagini in RAM...\")\n",
        "            for fname in tqdm(self.filenames, desc=\"Caricamento immagini\", unit=\"img\"):\n",
        "                img_path = os.path.join(self.img_dir, fname + \".png\")\n",
        "                image = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "                if self.transform:\n",
        "                    image = self.transform(image)\n",
        "\n",
        "                self.images.append(image)\n",
        "            print(f\"Caricate {len(self.images)} immagini in memoria.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.preload:\n",
        "            image = self.images[idx]\n",
        "        else:\n",
        "            img_path = os.path.join(self.img_dir, self.filenames[idx] + \".png\")\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # If label is not numeric, convert to class index\n",
        "        if isinstance(label, str):\n",
        "            # Optional: map string labels to integers\n",
        "            # You can build a mapping outside this class\n",
        "            raise ValueError(\"Labels are strings. Convert them to int first.\")\n",
        "\n",
        "        return image, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cpu')\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda') # Uncomment this to run on GPU\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_cuda_devices = torch.cuda.device_count()\n",
        "print(f\"CUDA è disponibile. Numero di dispositivi GPU: {num_cuda_devices}\")\n",
        "\n",
        "# (Opzionale) Stampa il nome di ciascun dispositivo\n",
        "for i in range(num_cuda_devices):\n",
        "    print(f\"Dispositivo {i}: {torch.cuda.get_device_name(i)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create datasets and loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Caricamento dataset sulla RAM (set FALSE se non vuoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZLrhMG_GTKG"
      },
      "outputs": [],
      "source": [
        "if local_runtime==1:\n",
        "    train_set_preload = True\n",
        "else:\n",
        "    train_set_preload = False\n",
        "\n",
        "train_set_binary = CustomImageDataset(augm_train_dir, labels_train_binary, transform=transform, preload=train_set_preload)\n",
        "train_loader_binary = DataLoader(train_set_binary, batch_size=batch_size, shuffle=True)\n",
        "val_set_binary = CustomImageDataset(processed_val_dir, labels_val_binary, transform=transform, preload=False)\n",
        "val_loader_binary = DataLoader(val_set_binary, batch_size=batch_size, shuffle=False)\n",
        "test_set_binary = CustomImageDataset(processed_test_dir, labels_test_binary, transform=transform, preload=False)\n",
        "test_loader_binary = DataLoader(test_set_binary, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_set_four_classes = CustomImageDataset(augm_train_dir, labels_train_four_classes, transform=transform, preload=train_set_preload)\n",
        "train_loader_four_classes = DataLoader(train_set_four_classes, batch_size=batch_size, shuffle=True)\n",
        "val_set_four_classes = CustomImageDataset(processed_val_dir, labels_val_four_classes, transform=transform, preload=False)\n",
        "val_loader_four_classes = DataLoader(val_set_four_classes, batch_size=batch_size, shuffle=False)\n",
        "test_set_four_classes = CustomImageDataset(processed_test_dir, labels_test_four_classes, transform=transform, preload=False)\n",
        "test_loader_four_classes = DataLoader(test_set_four_classes, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "GU4ryg7WIXlQ",
        "outputId": "042f893c-0c9e-4f64-b3aa-5999130fe1c6"
      },
      "outputs": [],
      "source": [
        "# Get one batch from the DataLoader\n",
        "images, labels = next(iter(train_set_four_classes))  # images: [B, 3, 224, 224], labels: [B]\n",
        "\n",
        "# Denormalize for display (undo ImageNet normalization)\n",
        "mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
        "std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
        "images = images * std + mean  # undo normalization\n",
        "\n",
        "# Make a grid of images\n",
        "grid = torchvision.utils.make_grid(images[:8], nrow=4)  # first 8 images, 4 per row\n",
        "\n",
        "# Convert to numpy for matplotlib (C,H,W -> H,W,C)\n",
        "grid_np = grid.permute(1, 2, 0).numpy()\n",
        "\n",
        "# Plot the images\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(grid_np)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Print corresponding labels\n",
        "print(\"Labels:\", labels[:8].tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Subset Loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In samples_per_class decidere quanti samples prendere per avere una distribuzione equa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def create_subset_loader(train_labels, train_set):\n",
        "    num_classes = 5\n",
        "    samples_per_class = 700\n",
        "\n",
        "    # Dictionary to hold indices for each class\n",
        "    class_indices = {c: [] for c in range(num_classes)}\n",
        "\n",
        "    # Iterate through dataset and collect indices by class\n",
        "    for idx, row in train_labels.iterrows():\n",
        "        label = int(row['diagnosis'])\n",
        "        class_indices[label].append(idx)\n",
        "\n",
        "    # For each class, randomly sample X indices\n",
        "    subset_indices = []\n",
        "    for c in range(num_classes):\n",
        "        chosen = np.random.choice(class_indices[c], samples_per_class, replace=False)\n",
        "        subset_indices.extend(chosen)\n",
        "\n",
        "    # Shuffle the final list of subset indices\n",
        "    np.random.shuffle(subset_indices)\n",
        "\n",
        "    # Create a subset of the dataset\n",
        "    subset_dataset = Subset(train_set, subset_indices)\n",
        "\n",
        "    # Create a new DataLoader for the subset\n",
        "    return DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if use_subset_loader == 1:\n",
        "    loader_selected_binary = create_subset_loader(train_labels = labels_train_binary, train_set = train_set_binary)\n",
        "    loader_selected_four_classes = create_subset_loader(train_labels = labels_train_four_classes, train_set = train_set_four_classes)\n",
        "else:\n",
        "    loader_selected_binary = train_loader_binary\n",
        "    loader_selected_four_classes = train_loader_four_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if use_subset_loader == 1:\n",
        "    labels_check = []\n",
        "    for _, label in loader_selected_binary:\n",
        "        labels_check.append(int(label))\n",
        "\n",
        "    print(\"Class distribution in binary subset:\", Counter(labels_check))\n",
        "\n",
        "    labels_check = []\n",
        "    for _, label in loader_selected_four_classes:\n",
        "        labels_check.append(int(label))\n",
        "\n",
        "    print(\"Class distribution in four classes subset:\", Counter(labels_check))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper function for model training and evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_optimizer(\n",
        "    name: str,\n",
        "    params,\n",
        "    lr: None\n",
        ") -> optim.Optimizer:\n",
        "    \"\"\"\n",
        "    Crea un optimizer da stringa. Esempi:\n",
        "    'adam', 'sgd', 'adamw', 'rmsprop', 'adagrad'\n",
        "    opt_kwargs: qualunque parametro aggiuntivo (es. weight_decay, momentum...)\n",
        "    \"\"\"\n",
        "    name = name.strip().lower()\n",
        "    if name == \"adam\":\n",
        "        if lr is None:\n",
        "            return optim.Adam(params)\n",
        "        else:\n",
        "            return optim.Adam(params, lr=lr)\n",
        "    if name == \"adamw\":\n",
        "        if lr is None:\n",
        "            return optim.AdamW(params)\n",
        "        else:\n",
        "            return optim.AdamW(params, lr=lr)\n",
        "    if name == \"sgd\":\n",
        "        if lr is None:\n",
        "            return optim.SGD(params)\n",
        "        else:\n",
        "            return optim.SGD(params, lr=lr)\n",
        "    if name == \"rmsprop\":\n",
        "        if lr is None:\n",
        "            return optim.RMSprop(params)\n",
        "        else:\n",
        "            return optim.RMSprop(params, lr=lr)\n",
        "    if name == \"adagrad\":\n",
        "        if lr is None:\n",
        "            return optim.Adagrad(params)\n",
        "        else:\n",
        "            return optim.Adagrad(params, lr=lr)\n",
        "    raise ValueError(f\"Optimizer sconosciuto: '{name}'\")\n",
        "\n",
        "def is_focal_loss(\n",
        "    name: str\n",
        ") -> nn.Module:\n",
        "    name = name.strip().lower()\n",
        "    if name in (\"focal\", \"focallos\"):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def build_criterion(\n",
        "    name: str\n",
        ") -> nn.Module:\n",
        "    \"\"\"\n",
        "    Crea una loss da stringa. Esempi:\n",
        "    'crossentropy', 'bcelogits', 'mse', 'nll', 'smoothl1'\n",
        "    loss_kwargs: parametri extra (es. weight, reduction, label_smoothing...)\n",
        "    \"\"\"\n",
        "    name = name.strip().lower()\n",
        "    if name in (\"crossentropy\", \"crossentropyloss\", \"ce\"):\n",
        "        return nn.CrossEntropyLoss()\n",
        "    if name in (\"bcelogits\", \"bcelosslogits\", \"bcelogitsloss\"):\n",
        "        return nn.BCEWithLogitsLoss()\n",
        "    if name in (\"bce\", \"bceloss\"):\n",
        "        return nn.BCELoss()\n",
        "    if name in (\"mse\", \"mseloss\", \"l2\"):\n",
        "        return nn.MSELoss()\n",
        "    if name in (\"nll\", \"nllloss\"):\n",
        "        return nn.NLLLoss()\n",
        "    if name in (\"smoothl1\", \"huber\"):\n",
        "        return nn.SmoothL1Loss()\n",
        "    if is_focal_loss(name):\n",
        "        return FocalLoss(gamma=2.0)\n",
        "    raise ValueError(f\"Criterion sconosciuto: '{name}'\")\n",
        "\n",
        "def build_scheduler(name: str, optimizer) -> nn.Module:\n",
        "    name = name.strip().lower()\n",
        "    if name in (\"reducelronplateau\"):\n",
        "        return ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',       # because we want to minimize val_loss\n",
        "            factor=0.3,       # reduce LR by 0.3\n",
        "            patience=2,       # wait 2 epochs before reducing LR\n",
        "        )\n",
        "    else:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multiclass_accuracy(outputs: torch.Tensor, targets: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Calcola l'accuracy per classificazione multi-classe (es. 5 classi).\n",
        "    outputs: tensor [N, num_classes]\n",
        "    targets: tensor [N]\n",
        "    \"\"\"\n",
        "    preds = outputs.argmax(dim=1)\n",
        "    correct = (preds == targets).sum().item()\n",
        "    total = targets.size(0)\n",
        "    return correct / max(1, total)\n",
        "\n",
        "def binary_accuracy(outputs: torch.Tensor, targets: torch.Tensor) -> float:\n",
        "    \"\"\"\n",
        "    Calcola l'accuracy per classificazione binaria.\n",
        "    outputs: tensor [N, 1] o [N]\n",
        "    targets: tensor [N] - etichette binarie (0 o 1)\n",
        "    \"\"\"\n",
        "    preds = (outputs >= 0.5).long().view(-1)\n",
        "    correct = (preds == targets.long()).sum().item()\n",
        "    total = targets.size(0)\n",
        "    return correct / max(1, total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader: torch.utils.data.DataLoader,\n",
        "    val_loader: None,\n",
        "    optimizer_name: \"adam\",\n",
        "    criterion_name: \"crossentropy\",\n",
        "    lr: None,\n",
        "    num_epochs: 5,\n",
        "    device: None,\n",
        "    scheduler_name = \"\",\n",
        "    verbose: bool = True,\n",
        "    model_output_softmax = False,\n",
        "    model_output_binary = False\n",
        ") -> Dict[str, List[float]]:\n",
        "    \"\"\"\n",
        "    Esegue training (e opzionalmente validazione) per num_epochs.\n",
        "    Ritorna uno storico con 'step_losses', 'epoch_losses', 'val_losses'.\n",
        "    \"\"\"\n",
        "    if device is None:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = build_criterion(criterion_name)\n",
        "    optimizer = build_optimizer(optimizer_name, model.parameters(), lr=lr)\n",
        "    uses_focal_loss = is_focal_loss(criterion_name)\n",
        "    scheduler = build_scheduler(scheduler_name, optimizer)\n",
        "\n",
        "    history = {\n",
        "        \"step_losses\": [],\n",
        "        \"epoch_losses\": [],\n",
        "        \"val_losses\": [],\n",
        "        \"epoch_acc\": [],\n",
        "        \"val_acc\": []\n",
        "    }\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "            train_iter = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)\n",
        "        else:\n",
        "            train_iter = enumerate(train_loader)\n",
        "\n",
        "        for _, (images, labels) in train_iter:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            if uses_focal_loss and not model_output_softmax:\n",
        "                loss = criterion(torch.softmax(outputs, dim=-1), labels)\n",
        "            else:\n",
        "                loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # --- metriche ---\n",
        "            if model_output_binary:\n",
        "                acc = binary_accuracy(outputs, labels)\n",
        "            else:\n",
        "                acc = multiclass_accuracy(outputs, labels)\n",
        "            correct_train += acc * labels.size(0)\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            history[\"step_losses\"].append(loss.item())\n",
        "\n",
        "            if verbose:\n",
        "                if hasattr(train_iter, \"set_description\"):\n",
        "                    train_iter.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "                if hasattr(train_iter, \"set_postfix\"):\n",
        "                    train_iter.set_postfix(loss=loss.item(), acc=acc)\n",
        "\n",
        "        avg_epoch_loss = epoch_loss / max(1, len(train_loader))\n",
        "        epoch_acc = correct_train / max(1, total_train)\n",
        "        history[\"epoch_losses\"].append(avg_epoch_loss)\n",
        "        history[\"epoch_acc\"].append(epoch_acc)\n",
        "\n",
        "        # Validazione (se fornito val_loader)\n",
        "        if val_loader is not None:\n",
        "            model.eval()\n",
        "            val_loss_total = 0.0\n",
        "            correct_val = 0\n",
        "            total_val = 0\n",
        "            n_val_batches = 0\n",
        "            with torch.no_grad():\n",
        "                val_iter = tqdm(val_loader, total=len(val_loader), leave=False) if verbose else val_loader\n",
        "                for X_val, Y_val in val_iter:\n",
        "                    X_val, Y_val = X_val.to(device), Y_val.to(device)\n",
        "                    Y_pred_val = model(X_val)\n",
        "\n",
        "                    if uses_focal_loss:\n",
        "                        loss_val = criterion(torch.softmax(Y_pred_val, dim=-1), Y_val)\n",
        "                    else:\n",
        "                        loss_val = criterion(Y_pred_val, Y_val)\n",
        "\n",
        "                    val_loss_total += loss_val.item()\n",
        "\n",
        "                    if model_output_binary:\n",
        "                        acc_val = binary_accuracy(Y_pred_val, Y_val)\n",
        "                    else:\n",
        "                        acc_val = multiclass_accuracy(Y_pred_val, Y_val)\n",
        "                    correct_val += acc_val * Y_val.size(0)\n",
        "                    total_val += Y_val.size(0)\n",
        "\n",
        "                    n_val_batches += 1\n",
        "\n",
        "            avg_val_loss = val_loss_total / max(1, n_val_batches)\n",
        "            val_acc = correct_val / max(1, total_val)\n",
        "            history[\"val_losses\"].append(avg_val_loss)\n",
        "            history[\"val_acc\"].append(val_acc)\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch+1} training loss: {avg_epoch_loss:.4f} train acc={epoch_acc:.4f}, validation loss: {avg_val_loss:.4f} validation acc={val_acc:.4f}\")\n",
        "        else:\n",
        "            # se non c'è validazione, manteniamo lunghezze allineate\n",
        "            history[\"val_losses\"].append(float('nan'))\n",
        "            history[\"val_acc\"].append(float('nan'))\n",
        "            if verbose:\n",
        "                print(f\"Epoch {epoch+1} training loss: {avg_epoch_loss:.4f} train acc={epoch_acc:.4f}\")\n",
        "\n",
        "        # Step dello scheduler (se presente)\n",
        "        if scheduler is not None:\n",
        "            # Alcuni scheduler richiedono val_loss (es. ReduceLROnPlateau)\n",
        "            if hasattr(scheduler, 'step') and scheduler.__class__.__name__.lower().startswith('reducelronplateau'):\n",
        "                last_val = history[\"val_losses\"][-1]\n",
        "                # usa train loss se non c'è validazione\n",
        "                metric = last_val if not (last_val != last_val) else avg_epoch_loss\n",
        "                scheduler.step(metric)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "\n",
        "    return history, model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_history(history, figsize=(15, 5), save_path=None, show=True):\n",
        "    \"\"\"\n",
        "    Mostra (e opzionalmente salva) tre grafici:\n",
        "      1. Step Losses (per batch)\n",
        "      2. Epoch Losses (train e validation)\n",
        "      3. Epoch Accuracy (train e validation)\n",
        "\n",
        "    Parametri\n",
        "    ----------\n",
        "    history : dict\n",
        "        Dizionario prodotto da train_model, con chiavi:\n",
        "            - \"step_losses\": perdite per batch\n",
        "            - \"epoch_losses\": perdite medie di training per epoca\n",
        "            - \"val_losses\": perdite medie di validazione per epoca\n",
        "            - \"epoch_acc\": accuracy di training per epoca\n",
        "            - \"val_acc\": accuracy di validazione per epoca\n",
        "    figsize : tuple\n",
        "        Dimensioni della figura (default: (15, 4))\n",
        "    save_path : str, opzionale\n",
        "        Se specificato, salva il grafico nel percorso indicato.\n",
        "    show : bool\n",
        "        Se True mostra il grafico (default: True)\n",
        "    title : str\n",
        "        Titolo generale della figura.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
        "\n",
        "    # --- Step losses (batch) ---\n",
        "    axes[0].plot(history[\"step_losses\"])\n",
        "    axes[0].set_title(\"Step Losses\")\n",
        "    axes[0].set_xlabel(\"Step\")\n",
        "    axes[0].set_ylabel(\"Loss\")\n",
        "\n",
        "    # --- Epoch losses (train + val) ---\n",
        "    axes[1].plot(history[\"epoch_losses\"], label=\"Training Loss\")\n",
        "    axes[1].plot(history[\"val_losses\"], label=\"Validation Loss\")\n",
        "    axes[1].set_title(\"Epoch Losses\")\n",
        "    axes[1].set_xlabel(\"Epoch\")\n",
        "    axes[1].set_ylabel(\"Loss\")\n",
        "    axes[1].legend()\n",
        "\n",
        "    axes[2].plot(history.get(\"epoch_acc\", []), label=\"Training Accuracy\", marker='o')\n",
        "    axes[2].plot(history.get(\"val_acc\", []), label=\"Validation Accuracy\", marker='o')\n",
        "    axes[2].set_title(\"Epoch Accuracy\")\n",
        "    axes[2].set_xlabel(\"Epoch\")\n",
        "    axes[2].set_ylabel(\"Accuracy\")\n",
        "    axes[2].set_ylim(0, 1)\n",
        "    axes[2].legend()\n",
        "    #axes[2].grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, dpi=300)\n",
        "    if show:\n",
        "        plt.show()\n",
        "\n",
        "    return fig, axes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(\n",
        "    model,\n",
        "    test_loader,\n",
        "    average: str = \"macro\",\n",
        "    show_confusion: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Valuta il modello sul test set e restituisce le metriche principali.\n",
        "\n",
        "    Parametri\n",
        "    ----------\n",
        "    model : torch.nn.Module\n",
        "        Il modello PyTorch da valutare.\n",
        "    test_loader : DataLoader\n",
        "        Dataloader del set di test.\n",
        "    average : str\n",
        "        Tipo di media per precision/recall/F1 (\"macro\", \"micro\", \"weighted\", \"binary\").\n",
        "    show_confusion : bool\n",
        "        Se True, stampa anche la matrice di confusione.\n",
        "\n",
        "    Ritorna\n",
        "    -------\n",
        "    metrics : dict\n",
        "        Dizionario con chiavi:\n",
        "        - \"accuracy\"\n",
        "        - \"precision\"\n",
        "        - \"recall\"\n",
        "        - \"f1\"\n",
        "        - \"confusion_matrix\" (se show_confusion=True)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # --- Calcolo metriche ---\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, average=average, zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, average=average, zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average=average, zero_division=0)\n",
        "\n",
        "    metrics = {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1,\n",
        "    }\n",
        "\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "    if show_confusion:\n",
        "        metrics[\"confusion_matrix\"] = cm\n",
        "        print(\"\\nConfusion Matrix:\\n\", cm)\n",
        "\n",
        "    print(f\"Test Accuracy: {acc * 100:.2f}% | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f}\")\n",
        "\n",
        "    return metrics, cm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cleanup_torch_env(model_name: None):\n",
        "    \"\"\"\n",
        "    Pulisce in sicurezza l'ambiente PyTorch:\n",
        "    - Sposta il modello su CPU (se esiste)\n",
        "    - Elimina l'optimizer (se esiste)\n",
        "    - Elimina eventuali variabili temporanee note\n",
        "    - Esegue garbage collection e svuota la cache GPU\n",
        "\n",
        "    Parametri\n",
        "    ----------\n",
        "    model_name : str\n",
        "        Nome della variabile modello da spostare su CPU.\n",
        "    optimizer_name : str\n",
        "        Nome della variabile optimizer da eliminare.\n",
        "    \"\"\"\n",
        "    # --- Sposta modello su CPU ---\n",
        "    if model_name in globals():\n",
        "        globals()[model_name] = globals()[model_name].cpu()\n",
        "    elif model_name in locals():\n",
        "        locals()[model_name] = locals()[model_name].cpu()\n",
        "\n",
        "    # --- Cancella variabili temporanee ---\n",
        "    temp_vars = [\n",
        "        'outputs', 'loss', 'images', 'labels',\n",
        "        'Y_pred_val', 'X_val', 'Y_val',\n",
        "        'sample_preds_labels', 'sample_labels',\n",
        "        'sample_images', 'sample_preds',\n",
        "        'all_labels', 'all_preds'\n",
        "    ]\n",
        "\n",
        "    for var in temp_vars:\n",
        "        if var in globals():\n",
        "            del globals()[var]\n",
        "        elif var in locals():\n",
        "            del locals()[var]\n",
        "\n",
        "    # --- Garbage collector e cache GPU ---\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Grid search helpers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definire in una griglia tutti i parametri che si vogliono testare: diversi optimizer (SGD, Adam, AdamW), diversi learning rates ecc. La funzione andrà a provare tutte le combinazioni possibili dei parametri inseriti nella griglia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "def fig_to_base64(fig) -> str:\n",
        "    \"\"\"Converte una figura Matplotlib in stringa base64 PNG e chiude la figura.\"\"\"\n",
        "    buf = io.BytesIO()\n",
        "    fig.savefig(buf, format=\"png\", dpi=200, bbox_inches=\"tight\")\n",
        "    buf.seek(0)\n",
        "    b64 = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
        "    plt.close(fig)\n",
        "    return b64\n",
        "\n",
        "def get_train_size(train_loader) -> int:\n",
        "    return len(train_loader.dataset)\n",
        "\n",
        "def run_experiments_to_single_csv(\n",
        "    model_fn: Callable[[], torch.nn.Module],\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    test_loader,\n",
        "    param_grid: Dict[str, List[Any]],\n",
        "    model_name: str = \"model\",\n",
        "    csv_folder = base_path+\"/results/\",\n",
        "    num_epochs: int = 5,\n",
        "    seed: int = 42,\n",
        "    device: Optional[torch.device] = None,\n",
        "    model_output_softmax = False\n",
        "    ):\n",
        "    os.makedirs(csv_folder, exist_ok=True)\n",
        "    \"\"\"\n",
        "    Esegue tutte le combinazioni e APPENDE una riga per run a un unico CSV.\n",
        "    La riga contiene: parametri, dimensioni train set, metriche test, cm (json), plot (base64).\n",
        "    \"\"\"\n",
        "    set_seed(seed)\n",
        "\n",
        "    keys = list(param_grid.keys())\n",
        "    combos = list(itertools.product(*(param_grid[k] for k in keys)))\n",
        "    print(f\"Partono {len(combos)} esperimenti...\\n\")\n",
        "\n",
        "    # Prepara CSV: scrivi header solo se non esiste\n",
        "    out_csv_path = csv_folder+model_name+\".csv\"\n",
        "    write_header = not os.path.exists(out_csv_path)\n",
        "\n",
        "    for i, values in enumerate(combos, 1):\n",
        "        params = {k: v for k, v in zip(keys, values)}\n",
        "        run_name = f\"{model_name}_run_{i:03d}_\" + \"_\".join(f\"{k}={v}\" for k, v in params.items())\n",
        "        print(f\"[{i}/{len(combos)}] {run_name}\")\n",
        "\n",
        "        # Nuovo modello per ogni run\n",
        "        model = model_fn()\n",
        "        train_size = get_train_size(train_loader)\n",
        "\n",
        "        # Train\n",
        "        history, model = train_model(\n",
        "                model=model,\n",
        "                train_loader=train_loader,\n",
        "                val_loader=val_loader,\n",
        "                optimizer_name=params[\"optimizer_name\"],\n",
        "                criterion_name=params[\"criterion_name\"],\n",
        "                scheduler_name=params[\"scheduler_name\"],\n",
        "                lr=params[\"lr\"],\n",
        "                num_epochs=num_epochs,\n",
        "                device=device,\n",
        "                model_output_softmax=model_output_softmax\n",
        "            )\n",
        "\n",
        "        # History -> plot -> base64 (niente salvataggi su disco)\n",
        "        plot_history(history, figsize=(15, 5), save_path=csv_folder+run_name+\".png\", show=False)\n",
        "\n",
        "        # Valutazione test (metriche + cm)\n",
        "        metrics, cm = evaluate_model(\n",
        "                model=model,\n",
        "                test_loader=test_loader,\n",
        "                average=\"macro\",\n",
        "                show_confusion=False\n",
        "            )\n",
        "\n",
        "        # Prepara riga per CSV (cm serializzata in JSON, plot come base64)\n",
        "        row = {\n",
        "                \"run_name\": run_name,\n",
        "                \"optimizer_name\": params[\"optimizer_name\"],\n",
        "                \"criterion_name\": params[\"criterion_name\"],\n",
        "                \"lr\": float(params[\"lr\"]),\n",
        "                \"num_epochs\": int(num_epochs),\n",
        "                \"train_size\": int(train_size),\n",
        "                \"seed\": int(seed),\n",
        "                \"device\": str(device),\n",
        "                \"test_accuracy\": float(metrics.get(\"accuracy\", float(\"nan\"))),\n",
        "                \"test_precision\": float(metrics.get(\"precision\", float(\"nan\"))),\n",
        "                \"test_recall\": float(metrics.get(\"recall\", float(\"nan\"))),\n",
        "                \"test_f1\": float(metrics.get(\"f1\", float(\"nan\"))),\n",
        "                \"confusion_matrix_json\": json.dumps(cm.tolist()),\n",
        "                \"graph_image_name\": str(run_name+\".png\")\n",
        "            }\n",
        "        \n",
        "        # Scrivi/append sul CSV\n",
        "        df = pd.DataFrame([row])\n",
        "        df.to_csv(out_csv_path, mode=\"a\", header=write_header, index=False)\n",
        "        write_header = False  # solo la prima volta\n",
        "\n",
        "        cleanup_torch_env(model)\n",
        "\n",
        "    print(f\"\\nTutte le run sono salvate in: {out_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_binary = {\n",
        "        \"optimizer_name\": [\"sgd\"],\n",
        "        \"lr\": [0.001],\n",
        "        \"criterion_name\": [\"bce\"],\n",
        "        \"scheduler_name\": [\"\"]\n",
        "    }\n",
        "\n",
        "grid_four_classes = {\n",
        "        #\"optimizer_name\": [\"sgd\", \"adam\", \"adamw\"],\n",
        "        #\"lr\": [1e-4, 1e-3, 5e-3],\n",
        "        #\"criterion_name\": [\"crossentropy\", \"focal\"],\n",
        "        #\"scheduler_name\": [\"\"]\n",
        "        \"optimizer_name\": [\"sgd\"],\n",
        "        \"lr\": [0.001],\n",
        "        \"criterion_name\": [\"crossentropy\"],\n",
        "        \"scheduler_name\": [\"\"]\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the first model (simple one), train and evaluate it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CCEE13wKsps",
        "outputId": "e0e11bad-f4c3-4e3f-ccee-06d1b678cda3"
      },
      "outputs": [],
      "source": [
        "# define the class\n",
        "\n",
        "class FMCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FMCNN, self).__init__()\n",
        "        # creating the layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, stride=1, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(in_features=64*int(img_size/4)*int(img_size/4), out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Employing the layers\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 64*int(img_size/4)*int(img_size/4))\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cM6wDlBrE4FV",
        "outputId": "5d222e3e-5f27-426c-c42c-889949d55e0b"
      },
      "outputs": [],
      "source": [
        "model = FMCNN()\n",
        "print(model)\n",
        "summary(model, input_size=(1, 3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTD52zTNOYbA",
        "outputId": "8605a21b-9430-4605-8d0e-0a819e2aaa72"
      },
      "outputs": [],
      "source": [
        "history, model = train_model(\n",
        "    model = model,\n",
        "    train_loader = loader_selected_four_classes,\n",
        "    val_loader = val_loader_four_classes,\n",
        "    criterion_name = \"crossentropy\",\n",
        "    optimizer_name = \"adam\",\n",
        "    num_epochs = 5,\n",
        "    device = device,\n",
        "    lr=None,\n",
        "    scheduler_name=\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "OSu_TQ8O7-Ad",
        "outputId": "dcafe4b3-5d00-4ff4-e936-4ebab4361f24"
      },
      "outputs": [],
      "source": [
        "plot_history(history = history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "id": "zZVpY0R_ZlIW",
        "outputId": "7ae7661a-bfac-4551-c9e0-0e51888c222e"
      },
      "outputs": [],
      "source": [
        "metrics, cm = evaluate_model(\n",
        "    model,\n",
        "    test_loader_four_classes,\n",
        "    show_confusion=True\n",
        ")\n",
        "\n",
        "# Make predictions on a few sample images\n",
        "sample_images, sample_labels = next(iter(test_loader_four_classes))\n",
        "sample_images = sample_images.to(device) # Move sample_images to the same device as the model\n",
        "sample_preds = model(sample_images)\n",
        "sample_preds_labels = torch.argmax(sample_preds, dim=1).cpu().numpy() # Move predictions back to CPU for numpy conversion\n",
        "\n",
        "print(\"Predicted labels:\", sample_preds_labels)\n",
        "print(\"True labels:\", sample_labels.numpy())\n",
        "\n",
        "imshow(torchvision.utils.make_grid(sample_images.cpu())) # Move images back to CPU for imshow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Libera la RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleanup_torch_env(\"model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Utilizzo della griglia dei parametri. Salva i risultati in un file .csv e le immagini nella cartella results "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "    return FMCNN()\n",
        "\n",
        "run_experiments_to_single_csv(\n",
        "        model_fn=model_fn,\n",
        "        train_loader=loader_selected_four_classes,\n",
        "        val_loader=val_loader_four_classes,\n",
        "        test_loader=test_loader_four_classes,\n",
        "        param_grid=grid_four_classes,\n",
        "        model_name=\"FMCNN\",\n",
        "        csv_folder = base_path+\"/results/\",\n",
        "        num_epochs=10,\n",
        "        device=device\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZMskfcz9uI7"
      },
      "source": [
        "## Test with another model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cvLMiFJ9zxe",
        "outputId": "127a6b47-3c3a-4abe-a60a-7a002d6e728f"
      },
      "outputs": [],
      "source": [
        "model_vgg16 = models.vgg16(weights=None, num_classes=5)\n",
        "\n",
        "print(model_vgg16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(model_vgg16, input_size=(batch_size, 3, img_size, img_size))\n",
        "#                               (batch_size, channels, H, W)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZp2BKfR_bWt",
        "outputId": "dc3edb44-ce9f-46e2-f52d-40a2c6fb93c5"
      },
      "outputs": [],
      "source": [
        "history, model_vgg16 = train_model(\n",
        "    model = model_vgg16,\n",
        "    train_loader = loader_selected_four_classes,\n",
        "    val_loader = val_loader_four_classes,\n",
        "    optimizer_name = \"adam\",\n",
        "    criterion_name = \"crossentropy\",\n",
        "    lr=1e-4,\n",
        "    num_epochs = 5,\n",
        "    device = device,\n",
        "    scheduler_name = \"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "O7o0f5ELBRA_",
        "outputId": "450f7021-26ce-46af-b176-d0a88ce83b68"
      },
      "outputs": [],
      "source": [
        "plot_history(history = history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model_vgg16, base_path + \"model_vgg.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 753
        },
        "id": "piw8rF_5BW2H",
        "outputId": "74a3e351-a498-4ff0-dee0-6f3b2399d57c"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "model2 = torch.load(base_path+\"model_vgg.pth\", weights_only=False)\n",
        "\n",
        "metrics, cm = evaluate_model(\n",
        "    model,\n",
        "    test_loader_four_classes,\n",
        "    show_confusion=True\n",
        ")\n",
        "\n",
        "# Make predictions on a few sample images\n",
        "sample_images, sample_labels = next(iter(test_loader_four_classes))\n",
        "sample_images = sample_images.to(device) # Move sample_images to the same device as the model\n",
        "sample_preds = model2(sample_images)\n",
        "sample_preds_labels = torch.argmax(sample_preds, dim=1).cpu().numpy() # Move predictions back to CPU for numpy conversion\n",
        "\n",
        "\n",
        "print(\"Predicted labels:\", sample_preds_labels)\n",
        "print(\"True labels:\", sample_labels.numpy())\n",
        "\n",
        "imshow(torchvision.utils.make_grid(sample_images.cpu())) # Move images back to CPU for imshow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleanup_torch_env(\"model2\")\n",
        "cleanup_torch_env(\"model_vgg16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "    return models.vgg16(weights=None, num_classes=5)\n",
        "\n",
        "run_experiments_to_single_csv(\n",
        "        model_fn=model_fn,\n",
        "        train_loader=loader_selected_four_classes,\n",
        "        val_loader=val_loader_four_classes,\n",
        "        test_loader=test_loader_four_classes,\n",
        "        param_grid=grid_four_classes,\n",
        "        model_name=\"vgg16\",\n",
        "        csv_folder = base_path+\"/results/\",\n",
        "        num_epochs=30,\n",
        "        device=device\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RSG-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RSGNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RSGNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, stride=1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, stride=1, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, stride=1, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, stride=1, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(in_features=128*20*20, out_features=128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=5)\n",
        "        self.gap = nn.AdaptiveMaxPool2d(20)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Employing the layers\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(x)\n",
        "        #x = self.pool(x)\n",
        "        x = self.gap(x)\n",
        "        x = x.view(-1, 128*20*20)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        #x = F.softmax(x, dim=1)\n",
        "        return x\n",
        "\n",
        "class RSGNet_Binary(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RSGNet_Binary, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, stride=1, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, stride=1, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, stride=1, kernel_size=3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, stride=1, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(in_features=128*20*20, out_features=128)\n",
        "        self.bn1 = nn.BatchNorm1d(128)\n",
        "        self.drop = nn.Dropout(p=0.3)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=1)\n",
        "        self.gap = nn.AdaptiveMaxPool2d(20)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Employing the layers\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.gap(x)\n",
        "        x = x.view(-1, 128*20*20)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Try a slightly different version, to try and overcome overfitting:\n",
        "- Early Stopping\n",
        "- LR scheduling: ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_rsg = RSGNet()\n",
        "print(model_rsg)\n",
        "summary(model_rsg, input_size=(batch_size, 3, img_size, img_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history, model_rsg = train_model(\n",
        "    model = model_rsg,\n",
        "    train_loader = train_loader_four_classes,\n",
        "    val_loader = val_loader_four_classes,\n",
        "    criterion_name = \"crossentropy\",\n",
        "    optimizer_name = \"sgd\",\n",
        "    lr=0.01,\n",
        "    num_epochs = 20,\n",
        "    device = device,\n",
        "    scheduler_name = \"reducelronplateau\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history, model_rsg = train_model(\n",
        "    model = model_rsg,\n",
        "    train_loader = train_loader_four_classes,\n",
        "    val_loader = val_loader_four_classes,\n",
        "    criterion_name = \"crossentropy\",\n",
        "    optimizer_name = \"sgd\",\n",
        "    lr=0.001,\n",
        "    num_epochs = 20,\n",
        "    device = device,\n",
        "    scheduler = None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_history(history = history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics, cm = evaluate_model(\n",
        "    model_rsg,\n",
        "    test_loader_four_classes,\n",
        "    show_confusion=True\n",
        ")\n",
        "\n",
        "# Make predictions on a few sample images\n",
        "sample_images, sample_labels = next(iter(test_loader_four_classes))\n",
        "sample_images = sample_images.to(device) # Move sample_images to the same device as the model\n",
        "sample_preds = model_rsg(sample_images)\n",
        "sample_preds_labels = torch.argmax(sample_preds, dim=1).cpu().numpy() # Move predictions back to CPU for numpy conversion\n",
        "\n",
        "\n",
        "print(\"Predicted labels:\", sample_preds_labels)\n",
        "print(\"True labels:\", sample_labels.numpy())\n",
        "\n",
        "imshow(torchvision.utils.make_grid(sample_images.cpu())) # Move images back to CPU for imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleanup_torch_env(\"model_rsg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "    return RSGNet()\n",
        "\n",
        "run_experiments_to_single_csv(\n",
        "        model_fn=model_fn,\n",
        "        train_loader=loader_selected_four_classes,\n",
        "        val_loader=val_loader_four_classes,\n",
        "        test_loader=test_loader_four_classes,\n",
        "        param_grid=grid_four_classes,\n",
        "        model_name=\"RSGNet\",\n",
        "        csv_folder = base_path+\"/results/\",\n",
        "        num_epochs=30,\n",
        "        device=device,\n",
        "        model_output_softmax = False\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_resnet18 = models.resnet18(weights='DEFAULT')\n",
        "print(model_resnet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_resnet18 = models.resnet18(weights='IMAGENET1K_V1')\n",
        "print(model_resnet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(type(model_resnet18))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Replace the last layer (\"fc\") with number of classes (5)\n",
        "model_resnet18.fc = nn.Linear(model_resnet18.fc.in_features, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "summary(model_resnet18, input_size=( batch_size, 3, img_size, img_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history, model_resnet18 = train_model(\n",
        "    model = model_resnet18,\n",
        "    train_loader = train_loader_four_classes,\n",
        "    val_loader = val_loader_four_classes,\n",
        "    criterion_name = \"crossentropy\",\n",
        "    optimizer_name = \"sgd\",\n",
        "    lr=0.01,\n",
        "    num_epochs = 20,\n",
        "    device = device,\n",
        "    scheduler_name = \"reducelronplateau\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simone's Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Simone(nn.Module):                                                                                                                                                                                \n",
        "    def __init__(self):\n",
        "        super(Simone, self).__init__()\n",
        "        # creating the layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, stride=1, kernel_size=3, padding=1)\n",
        "        self.dropout1 = nn.Dropout(p=0.2)  # 20% di dropout\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.dropout2 = nn.Dropout(p=0.2)  # 20% di dropout\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(in_features=128*int(img_size/4)*int(img_size/4), out_features=128)\n",
        "        self.dropout3 = nn.Dropout(p=0.5)  # 50% di dropout\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Employing the layers\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv3(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.conv4(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 128*int(img_size/4)*int(img_size/4))\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout3(x)  # applico dropout solo durante il training\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_simone = Simone()\n",
        "print(model_simone)\n",
        "summary(model_simone, input_size=(batch_size, 3, img_size, img_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history, model_simone = train_model(\n",
        "    model = model_simone,\n",
        "    train_loader = train_loader_four_classes,\n",
        "    val_loader = val_loader_four_classes,\n",
        "    criterion_name = \"crossentropy\",\n",
        "    optimizer = \"adam\",\n",
        "    lr=1e-3,\n",
        "    num_epochs = 30,\n",
        "    device = device,\n",
        "    scheduler_name=\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_history(history = history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics, cm = evaluate_model(\n",
        "    model_simone,\n",
        "    test_loader_four_classes,\n",
        "    show_confusion=True\n",
        ")\n",
        "\n",
        "# Make predictions on a few sample images\n",
        "sample_images, sample_labels = next(iter(test_loader_four_classes))\n",
        "sample_images = sample_images.to(device) # Move sample_images to the same device as the model\n",
        "sample_preds = model_simone(sample_images)\n",
        "sample_preds_labels = torch.argmax(sample_preds, dim=1).cpu().numpy() # Move predictions back to CPU for numpy conversion\n",
        "\n",
        "\n",
        "print(\"Predicted labels:\", sample_preds_labels)\n",
        "print(\"True labels:\", sample_labels.numpy())\n",
        "\n",
        "imshow(torchvision.utils.make_grid(sample_images.cpu())) # Move images back to CPU for imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleanup_torch_env(\"model_simone\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "    return Simone()\n",
        "\n",
        "run_experiments_to_single_csv(\n",
        "        model_fn=model_fn,\n",
        "        train_loader=loader_selected_four_classes,\n",
        "        val_loader=val_loader_four_classes,\n",
        "        test_loader=test_loader_four_classes,\n",
        "        param_grid=grid_four_classes,\n",
        "        model_name=\"Simone\",\n",
        "        csv_folder = base_path+\"/results/\",\n",
        "        num_epochs=30,\n",
        "        device=device\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Simone2's Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Simone2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.Conv2d(32,32,3,padding=1),   nn.BatchNorm2d(32), nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32,64,3,padding=1),   nn.BatchNorm2d(64), nn.ReLU(),\n",
        "            nn.Conv2d(64,128,3,padding=1),  nn.BatchNorm2d(128), nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.gap = nn.AdaptiveAvgPool2d(10)  # output: (B,128,1,1)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128*28*28, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 5)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.body(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.head(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_simone2 = Simone2()\n",
        "print(model_simone2)\n",
        "summary(model_simone2, input_size=(batch_size, 3, img_size, img_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history, model_simone2 = train_model(\n",
        "    model = model_simone2,\n",
        "    train_loader = train_loader_four_classes,\n",
        "    val_loader = val_loader_four_classes,\n",
        "    criterion_name = \"crossentropy\",\n",
        "    optimizer = \"adam\",\n",
        "    num_epochs = 100,\n",
        "    device = device,\n",
        "    scheduler_name=\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_history(history = history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics, cm = evaluate_model(\n",
        "    model,\n",
        "    test_loader_four_classes,\n",
        "    show_confusion=True\n",
        ")\n",
        "\n",
        "# Make predictions on a few sample images\n",
        "sample_images, sample_labels = next(iter(test_loader_four_classes))\n",
        "sample_images = sample_images.to(device) # Move sample_images to the same device as the model\n",
        "sample_preds = model_simone2(sample_images)\n",
        "sample_preds_labels = torch.argmax(sample_preds, dim=1).cpu().numpy() # Move predictions back to CPU for numpy conversion\n",
        "\n",
        "\n",
        "print(\"Predicted labels:\", sample_preds_labels)\n",
        "print(\"True labels:\", sample_labels.numpy())\n",
        "\n",
        "imshow(torchvision.utils.make_grid(sample_images.cpu())) # Move images back to CPU for imshow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cleanup_torch_env(\"model_rsg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "    return Simone2()\n",
        "\n",
        "run_experiments_to_single_csv(\n",
        "        model_fn=model_fn,\n",
        "        train_loader=loader_selected_four_classes,\n",
        "        val_loader=val_loader_four_classes,\n",
        "        test_loader=test_loader_four_classes,\n",
        "        param_grid=grid_four_classes,\n",
        "        model_name=\"Simone2\",\n",
        "        csv_folder = base_path+\"/results/\",\n",
        "        num_epochs=5,\n",
        "        device=device\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Fn42OzLnZB-u"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
